{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on linear dynamical systems\n",
    "\n",
    "In this notebook, we'll go over examples of different linear dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "import ssm\n",
    "from ssm import util\n",
    "import LDS\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable relative imports\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import plotting_util\n",
    "import tutorial_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data from a linear dynamical system\n",
    "First, we generate states and observations from a linear dynamical system with gaussian noise. The dynamical system has the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{x}_{t+1} = A\\vec{x}_t + \\vec{\\omega}, \\text{where}~\\vec{\\omega} \\sim N(0,Q)\n",
    "\\end{equation}\n",
    "\n",
    "With observations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{y}_{t+1} = C\\vec{x}_{t+1} + \\vec{\\eta}, \\text{where}~\\vec{\\eta} \\sim N(0,R)\n",
    "\\end{equation}\n",
    "\n",
    "The initial state $\\vec{x}_0$ also comes from a gaussian distribution $N(\\pi_0,V_0)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `glds_generate_states_and_observations` below. **Fill in the code** to generate new latent states and observations. A few useful numpy functions:\n",
    "\n",
    "- [np.matmul(A,b)](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.dot.html): matrix multiplication between A and b, i.e. $A\\vec{b}$\n",
    "- [np.random.multivariate_normal($\\mu$, $\\Sigma$)](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.dot.html): generate a sample $\\vec{\\omega} \\sim N(\\mu,\\Sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glds_generate_states_and_observations(A, C, Q, R, pi_0, V_0, num_timesteps, seed=None):\n",
    "    \"\"\" Generates a sequence of states and observations from a gaussian linear dynamical system.\n",
    "\n",
    "    Args:\n",
    "        A (np.matrix): Dynamics matrix (d_latent, d_latent)\n",
    "        Q (np.matrix): State noise covariance (d_latent, d_latent)\n",
    "        C (np.matrix): Observation matrix (d_observation, d_latent)\n",
    "        R (np.matrix): Observation noise covariance (d_observation, d_observation)\n",
    "        pi_0 (np.array): Initial state mean (d_latent, )\n",
    "        V_0 (np.matrix): Initial state covariance (d_latent, d_latent)\n",
    "        num_timesteps (int, optional): number of iterations for EM\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): (num_timesteps, d_latent) time-series of states\n",
    "        Y (np.ndarray): (num_timesteps, d_observations) time-series of observations\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed) # for testing; do not change\n",
    "    assert A.shape[0] == A.shape[1], \"Dynamics matrix must be square\"\n",
    "    assert Q.shape[0] == Q.shape[1], \"State noise covariance must be square\"\n",
    "    assert C.shape[1] == A.shape[1], \"Number of columns in observation matrix must match d_latent \"\n",
    "    assert R.shape[0] == R.shape[1], \"Observation noise covariance must be square\"\n",
    "    \n",
    "    d_latent = A.shape[0]\n",
    "    d_observation = C.shape[0]\n",
    "\n",
    "    X = [] # list of states \n",
    "    Y = [] # list of observations\n",
    "    \n",
    "    # use these!\n",
    "    state_noise_mean = np.zeros((d_latent,))\n",
    "    observation_noise_mean = np.zeros((d_observation,))\n",
    "    \n",
    "    # generate initial state and observation\n",
    "    x = np.random.multivariate_normal(pi_0, V_0)\n",
    "    y = C.dot(x) + np.random.multivariate_normal(observation_noise_mean, R)\n",
    "    \n",
    "    # add x and y to their respective lists\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    \n",
    "\n",
    "    \n",
    "    for _ in range(1, num_timesteps):\n",
    "        \"\"\"TODO: your code goes here! Fill in the formulas for x and y.\n",
    "        \n",
    "        Note: just as we did in the initialization, we're actually finding x_{t+1} and y_{t+1} at each timestep.\n",
    "        This means that your equation for y should be in terms of the _new_ x, not the previous x.\n",
    "        \"\"\"\n",
    "    \n",
    "        x = 0\n",
    "        y = 0\n",
    "        \n",
    "        \"\"\"End your code\"\"\"\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to test your implementation\n",
    "tutorial_util.test_implementation(glds_generate_states_and_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing linear dynamical systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see a few different ways of visualizing linear dynamical systems. Let's initialize some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the gLDS\n",
    "d_latent = 2\n",
    "d_observation = 3\n",
    "A = .95* util.random_rotation(d_latent,theta=-np.pi/6) # dynamics matrix, a slowly decaying rotation\n",
    "C = np.random.rand(d_observation,d_latent) # observation matrix, random\n",
    "\n",
    "# we'll set the covariances to be diagonal\n",
    "Q = np.diag(np.random.rand(d_latent,)) # state noise covariance\n",
    "R = np.diag(np.random.rand(d_observation,)) # observation noise covariance\n",
    "\n",
    "pi_0 = np.zeros((d_latent,)) # initial state mean\n",
    "V_0 = Q # initial state covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_vector_field(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these parameters and the function we wrote above, let's generate 200 timesteps of states $x_t$ and observations $y_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 200\n",
    "X, Y = glds_generate_states_and_observations(A, C, Q, R, pi_0, V_0, num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot each dimension of the latent state separately over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_latents_over_time(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing for the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_observations_over_time(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the latents is by looking at the phase space -- now, our axes are the values of the latent dimensions, and our plot evolves over time. We've linked the phase space to the observations so we can see how the two relate to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "anim = plotting_util.scatter_animation_2D_and_3D(X, Y)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize the dynamics with a flow field. The arrows show how, in the absence of noise, the latent state evolves over time. For example, here's the flow field when our dynamics matrix $A$ is a rotation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.pi/6 # angle of rotation in radians\n",
    "rotation_matrix = np.matrix([[np.cos(theta), np.sin(theta)],\n",
    "                   [-np.sin(theta),np.cos(theta)]])\n",
    "plotting_util.plot_vector_field(rotation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are flow fields when our dynamics matrices collapse the state or expand the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsing_dynamics = np.matrix([[.9, 0],\n",
    "                                [0,.9]])\n",
    "expanding_dynamics = np.matrix([[1.1, 0],\n",
    "                               [0,1.1]])\n",
    "plotting_util.plot_vector_field(collapsing_dynamics, expanding_dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, we can figure out which of these qualitative behaviors will occur based on the eigenvalues of the dynamics matrices,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_eigenvalues = np.linalg.eig(rotation_matrix)[0]\n",
    "print(\"Eigenvalues of rotation matrix: {}\".format(rotation_eigenvalues))\n",
    "\n",
    "collapse_eigenvalues = np.linalg.eig(collapsing_dynamics)[0]\n",
    "print(\"Eigenvalues of collapsing matrix: {}\".format(collapse_eigenvalues))\n",
    "\n",
    "expanding_eigenvalues = np.linalg.eig(expanding_dynamics)[0]\n",
    "print(\"Eigenvalues of expanding matrix: {}\".format(expanding_eigenvalues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these qualitative behaviors to get rotating + collapsing dynamics, rotating + expanding dynamics, or collapsing + expanding (along different dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotating_collapse = collapsing_dynamics.dot(rotation_matrix)\n",
    "rotating_expand = expanding_dynamics.dot(rotation_matrix)\n",
    "plotting_util.plot_vector_field(rotating_collapse, rotating_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "Fill in the entries of this matrix to generate a matrix that collapses along one dimension and expands along the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these lines\n",
    "# collapsing_expand = np.matrix([[?,?],\n",
    "#                                [?,?]])\n",
    "# plotting_util.plot_vector_field(collapsing_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Linear Dynamical Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll see a simple example of fitting a linear dynamical system. We'll start with a toy example using the observations that we generated above, and learn the parameters just from these observations.\n",
    "\n",
    "The actual fitting work is happening in the [LDS.py](LDS.py) module. When we want to fit a dynamical system to a set of observations, we call the module's fit() function, which looks like this:\n",
    "\n",
    "```python\n",
    "for _ in range(num_iterations):\n",
    "    A, C, Q, R, pi_0, V_0 = self._m_step(x_s, V_s, ...)\n",
    "    x_s, V_s, ... = self._e_step(y, A, C, Q, R, pi_0, V_0)\n",
    "\n",
    "```\n",
    "\n",
    "The fit function implements EM, which alternates between 2 steps:\n",
    "    - E step (given current estimate of parameters, find most likely latent states)\n",
    "    - M step (given current estimate of latent states, find most likely parameters)\n",
    "    \n",
    "    \n",
    "Note: [LDS.py](LDS.py) is mostly for educational purposes only; it implements [Parameter estimation for linear dynamical systems](http://mlg.eng.cam.ac.uk/zoubin/course04/tr-96-2.pdf), but is generally only suitable for toy examples. [SSM](https://github.com/slinderman/ssm), a much more robust library, is presented later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDS.gLDS() # instantiate gaussian linear dynamical systems module\n",
    "x_hat, model_likelihood = model.fit(Y, d_latent, num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_likelihood = LDS.gLDS.get_likelihood(Y, A, C, Q, R, pi_0, V_0)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.hlines(true_likelihood, 0, len(model_likelihood), label='true')\n",
    "ax.plot(model_likelihood, label='model')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Log likelihood')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model likelihood can pass the \"true\" likelihood, because there could be other parameters that were more likely to have produced the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the dynamics matrix found by the model to the true dynamics matrix by looking at the flow fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_vector_field(model.A, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the estimated dynamics may be the mirror image of the 'true' dynamics because both can give rise to the same set of observations as if the columns of the model's C have their signs flipped relative to the 'true' C. What matters is that the eigenvalues are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "print(LA.eigvals(A))\n",
    "print(LA.eigvals(model.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the estimated latent states vs. the true latent states. Here are the true states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_latents_over_time(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here are the estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_latents_over_time(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So hopefully you're now convinced that it's possible to fit a linear dynamical system from data. Now that we've gone over a simple case, we'll dive into an example with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting dynamical systems to C. elegans postures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the basics of fitting and visualizing dynamical systems, we'll go over an example using real data. We'll be working with a dataset of C. elegans behavior from Andre Brown's [Behavioural Genomics Lab](http://behave.lms.mrc.ac.uk/). The videos have been preprocessed using the [Tierpsy Tracker](http://ver228.github.io/tierpsy-tracker/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import worm_util\n",
    "worm_data = worm_util.load_worm_data()\n",
    "for key in worm_data.keys():\n",
    "    print(key, \"shape: {}\".format(worm_data[key].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coordinates` field shows the inferred coordinates of the worm as it crawls around the plate. The worm is segmeneted into 48 different sections, and we have the x, y coordinates of each of the segment endpoints. Here's an animation of the worm's extracted position and body as it moves on the plate (note -- a camera is tracking the worm, hence the jumps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 300\n",
    "anim = plotting_util.plot_crawling_worm(worm_data['coordinates'][:num_frames], worm_data['fps'])\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a dynamical systems model of how the worm's `posture` changes over time. To get the posture, we center the worm and set the mean angle of it's body equal to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plotting_util.plot_crawling_worm(worm_data['posture'][:num_frames], worm_data['fps'])\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen worms\n",
    "\n",
    "Before fitting the dynamical systems model, we're going to reduce the dimensionality of our dataset from 48 joint segments to 7 using PCA. [Stephens et. al 2008](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000028) showed that 95% of the variance in C. elegans posture is explained by the top 4 principal components, or 'eigenworms'. Also, following Stephens et. al, the eigen worms are represented in terms of the tangent angles along the segments of the worm's body. We've kept the top 7 PCs which explain 98% of the variance in the postures. Here's what the eigenworms look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_eigen_worms(worm_data['eigen_worms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posture at any given time will be a linear combination of the seven eigenworms. Here are all of the `eigen_projections` for the full movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_eigen_projections(worm_data['eigen_projections'], worm_data['fps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first two trajectories plotted against one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plotting_util.scatter_animation_2D(worm_data['eigen_projections'][:90,1:3])\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to model these dynamics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-space modeling library\n",
    "\n",
    "Luckily, there are several open source libraries available for fitting state space models. We're going to use [SSM: Bayesian learning and inference for state space models](https://github.com/slinderman/ssm), a library by Scott Linderman (joining SNI and the Statistics department June 2019!). SSM provides a simple interface for fitting a number of different state space models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm.models import HMM\n",
    "from ssm.util import find_permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching linear dynamical system\n",
    "\n",
    "*Note: This model is directly taken from \"Quantifying the behavioral dynamics of C. elegans with autoregressive hidden Markov models\", a 2017 NeurIPS workshop paper by Buchanan, E. K., Lipschitz, A., Linderman, S. W., & Paninski, L.*\n",
    "\n",
    "We're going to fit a specific type of linear dynamical system called an auto-regressive hidden markov model (AR-HMM). The AR-HMM differs from the LDS model above in two crucial ways:\n",
    "\n",
    "1. Instead of having one dynamics matrix $A$, we'll $K$ different dynamics matrices $A_1, \\ldots, A_K$. We can think of the $K$ different sets of dynamics corresponding to $K$ different behaviors, each of which we model as being linear.\n",
    "\n",
    "2. For simplicity, we're going to drop the observations for now, and only try to model the dynamics of the eigen projections. We could have instead fit a full switching linear dynamical system where the observations were the original 48-dimensional vector...but that takes more time to fit. Try it on your own!\n",
    "\n",
    "The equations for the AR-HMM are as follows: we have the discrete states\n",
    "\n",
    "$$z _ { t + 1 } \\left| z _ { t } , \\left\\{ \\pi _ { k } \\right\\} _ { k = 1 } ^ { K } \\sim \\pi _ { z _ { t } }\\right.$$\n",
    "\n",
    "where is $\\left\\{ \\pi _ { k } \\right\\} _ { k = 1 } ^ { K }$ the Markov transition matrix and $\\pi _ { k } \\in [ 0,1 ] ^ { K }$ corresponds to the $k$-th row. Given discrete state $z_t$, the postural dynamics are given by\n",
    "\n",
    "$$x _ { t } \\left| x _ { t - 1 } , z _ { t } \\sim \\mathcal { N } \\left( A _ { z _ { t } } x _ { t - 1 } + b _ { z _ { t } } , Q _ { z _ { t } } \\right)\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters of the HMM\n",
    "T = worm_data['eigen_projections'].shape[0]    # number of time bins\n",
    "K = 4       # number of discrete states\n",
    "D = worm_data['eigen_projections'].shape[1]       # data dimension\n",
    "\n",
    "# Make an HMM, use direchlet prior on discrete states so high prob of staying in current state\n",
    "hmm = HMM(K, D, observations=\"ar\", transitions='sticky', transition_kwargs={'kappa': np.power(10,4)})\n",
    "hmm.initialize(worm_data['eigen_projections'])\n",
    "hmm_lls = hmm.fit(worm_data['eigen_projections'], method=\"em\", num_em_iters=20)\n",
    "\n",
    "plt.plot(hmm_lls, label=\"EM\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the inferred discrete states\n",
    "hmm_z = hmm.most_likely_states(worm_data['eigen_projections'])\n",
    "plotting_util.plot_discrete_states(hmm_z, worm_data['max_time_in_seconds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the AR-HMM learned any interested structure in our data. First, we can show a histogram of how the different discrete states correspond to hand-labeled behavioral states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_util.plot_discrete_state_behavioral_histograms(hmm_z, worm_data['mode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like one discrete state roughly corresponds to backwards crawling, the ~2 are forwards crawling, and the last might be pausing. \n",
    "\n",
    "We can also visualze the flowfields of the corresponding dynamics. Because the dynamics are in a 7-dimensional space, we visualize the dynamics projected onto the 2D subspace spanned by PCs 2 and 3 (eigen_worms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xkcd = np.array(plotting_util.xkcd_colors())\n",
    "for z in range(K):\n",
    "    A = hmm.observations.params[0][z,1:3,1:3]\n",
    "    plotting_util.plot_vector_field(A, color=xkcd[z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, now that we have a generative model of the worms behavior, we can sample from the hmm to simulate new worm experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_z, sampled_eigen_projections = hmm.sample(600)\n",
    "sampled_worm_postures = worm_util.angles_to_skeleton(worm_data['eigen_worms'].dot(sampled_eigen_projections.T)).transpose(1,0,2)\n",
    "# color worm by discrete state\n",
    "\n",
    "z = xkcd[sampled_z,:]\n",
    "anim = plotting_util.plot_crawling_worm(sampled_worm_postures, worm_data['fps'],colors=z)\n",
    "HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
